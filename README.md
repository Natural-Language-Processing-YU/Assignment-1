# Natural Language Processing - Assignment #1

## Overview
Much of this class reliese on the [https://web.stanford.edu/~jurafsky/slp3/ed3book_dec302020.pdf] Jurasky D. and Martin J. (2020) text as well as the Natural Language Toolkit [https://www.nltk.org/book/] Python library and text, so it is highly recommended that you read the prescribed chapters for each assignment and use that as the basis for the exercises. 

The first section of the class focuses on computational linguistics, text processing, basic language models. Before we start "digging deep" into the neural network processes for NLP -- the most efficient and now commonly used portion of NLP -- we will learn the predecessor algorithms and approaches to NLP. Often, you will find that it is important to understand how text is processed and managed to be fed into algorithms that would have rules to provide some categorization or classification of the output, assist in document similarity and searching, faciltate the translation of information, or even extract information from a corpus. 

Thus, the majority of the body of educational resources with NLP will start with teaching basic text processing such as regular expressions, stemming and lemmatizing words, stop words, and tokenization within a corpus. Subsequently, shallow learning technqiues where language modeling is at the fore uses statistical and probalistic approaches such as logistic regression, naive Bayes, and vectorization of words to support models like Continuous Bag of Words (CBOW), N-grams, GloVe, among other approaches. These are critical technqiues for supporting some of the use cases for NLP mentioned above. That being said, as availability of data became more widely available, and computer processing power both enhanced technological while simultaneously becoming much less expensive in terms of availability. 
